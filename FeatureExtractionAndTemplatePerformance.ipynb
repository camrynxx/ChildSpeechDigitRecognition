{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd8f40e",
   "metadata": {},
   "source": [
    "# Compare Feature Extraction Type (MFCCs and WavLM) and compare Template Type (Child, Adult, VC Adult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb19e6c",
   "metadata": {},
   "source": [
    "## Setup of Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53980c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camrynabrahamson/Documents/Engineering/Fourth Year/Skripsie/GitCoding/Evaluation/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/camrynabrahamson/Documents/Engineering/Fourth Year/Skripsie/GitCoding/Evaluation/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from os import path\n",
    "import os, glob, torch, torchaudio, re\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import mfcc\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import speech_dtw.qbe as qbe\n",
    "\n",
    "from transformers import WavLMModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(path.join(\"..\", \"utils\"))\n",
    "\n",
    "SAMPLE_RATE = 16000 \n",
    "WAVLM_LAYER_INDEX = 6\n",
    "device = \"cpu\"\n",
    "model = WavLMModel.from_pretrained(\"microsoft/wavlm-large\").to(device).eval()\n",
    "\n",
    "def cmvn(X):\n",
    "    # X: [T, D] NumPy\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + 1e-8)\n",
    "\n",
    "def getMFCCsFeatures(file): #A function which extracts MFCCs features from a given audio file\n",
    "    sig, rate = torchaudio.load(file) #Reads the audio file, extracting the sample rate and signal data (as an array)\n",
    "    if rate != SAMPLE_RATE: #Check if sampled as correct sampling rate, if not - resample\n",
    "        print(\"Resampling\", file ,\"at 16kHz.\\n\")\n",
    "        sig = torchaudio.functional.resample(sig, rate, SAMPLE_RATE)\n",
    "    sig = sig.squeeze(0).numpy()\n",
    "    MFCC_static = mfcc(sig, SAMPLE_RATE) #Extracts MFCCs features given\n",
    "    MFCC_deltas = delta(MFCC_static, 2) #Calculates delta (first derivative) of MFCCs features\n",
    "    MFCC_delta_deltas = delta(MFCC_deltas, 2) #Calculates delta-delta (second derivative) of MFCCs features\n",
    "    features = np.hstack((MFCC_static, MFCC_deltas, MFCC_delta_deltas)) #Combine static, delta, and delta-delta features into a single feature vector\n",
    "    features = cmvn(features) #Applies cepstral mean and variance normalization to features\n",
    "    return features\n",
    "\n",
    "def getWavLMFeatures(file): #A function which extracts MFCCs features from a given audio file\n",
    "    sig, rate = torchaudio.load(file) #Reads the audio file, extracting the sample rate and signal data (as an array)\n",
    "    if rate != SAMPLE_RATE: #Check if sampled as correct sampling rate, if not - resample\n",
    "        print(\"Resampling\", file ,\"at 16kHz.\\n\")\n",
    "        sig = torchaudio.functional.resample(sig, rate, SAMPLE_RATE)\n",
    "    sig = sig.to(device)\n",
    "    with torch.inference_mode(): #Extracts layer 6 features\n",
    "        out = model(sig, output_hidden_states=True)\n",
    "        features = out.hidden_states[WAVLM_LAYER_INDEX].squeeze(0)  # [T, D] torch\n",
    "    features = features.numpy() #Convert to numpy\n",
    "    features = cmvn(features) #Applies cepstral mean and variance normalization to features\n",
    "    return features\n",
    "\n",
    "def getFeatures(file, feature_type=\"wavlm\"):\n",
    "    if feature_type == \"mfcc\":\n",
    "        return getMFCCsFeatures(file)\n",
    "    else:\n",
    "        return getWavLMFeatures(file)\n",
    "\n",
    "def getMinimumCost(queryFile, templateFile, feature_type=\"wavlm\"): #Loading the features\n",
    "    queryFeatures = getFeatures(queryFile, feature_type) #Extract features for query data\n",
    "    templateFeatures = torch.load(templateFile)[\"features\"].numpy() #Load the template's feature file\n",
    "    queryFeatures = np.ascontiguousarray(queryFeatures, dtype=np.float64) #Make both feature sets 2D, float64, contiguous\n",
    "    templateFeatures = np.ascontiguousarray(templateFeatures, dtype=np.float64)\n",
    "    distance = qbe.dtw_sweep_min(queryFeatures, templateFeatures) #Calculate the minimum sweeping DTW distance between the two feature sets\n",
    "    return distance\n",
    "\n",
    "def predict(queryFile, templateFolder, feature_type=\"wavlm\"):\n",
    "    distances = [] #This will store tuples of (distance, label)\n",
    "    for templateFile in Path(templateFolder).rglob(\"*.pt\"): #Loop through all template feature files\n",
    "        distance = getMinimumCost(queryFile, str(templateFile), feature_type) #Get the minimum cost between the query and template\n",
    "        label = torch.load(templateFile)[\"label\"] #Extract the label of the template\n",
    "        distances.append((distance, label)) #Append the distance and label as a tuple to the distances list\n",
    "    distances.sort(key=lambda x: x[0]) #Sort the distances list by distance (first element of tuple) (using a lambda function)\n",
    "    predicted_label = distances[0][1] #Minimum cost prediction (k=1)\n",
    "    return predicted_label\n",
    "\n",
    "def getAccuracy(testFolder, templateFolder, feature_type=\"wavlm\"):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for testFile in Path(testFolder).rglob(\"*.wav\"): #Loop through all test audio files\n",
    "        if not re.match(r\"^\\d{2}_\\d{2}\\.wav$\", testFile.name): # skip files not matching NN_NN.wav\n",
    "            continue\n",
    "        true_label = int(testFile.stem.split(\"_\")[0]) #Extract the true label from the filename\n",
    "        predicted_label = predict(str(testFile), templateFolder, feature_type) #Predict the label using minimum cost\n",
    "        if predicted_label == true_label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ed110",
   "metadata": {},
   "source": [
    "## Evaluation for Template Types and Feature Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5c378",
   "metadata": {},
   "source": [
    "WavLM Accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd72b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WavLM Adult Template Accuracy: 58.82%\n",
      "WavLM AdultVC Template Accuracy: 60.78%\n",
      "WavLM Child Template Accuracy: 65.69%\n"
     ]
    }
   ],
   "source": [
    "accuracyWavLMAdult = getAccuracy(\"ValidationData/OnlyNumbers/\", \"TrainingData/TrainingFeatures/WavLM/Adult\", feature_type=\"wavlm\")\n",
    "print(f\"WavLM Adult Template Accuracy: {accuracyWavLMAdult:.2f}%\")\n",
    "accuracyWavLMAdultVC = getAccuracy(\"ValidationData/OnlyNumbers/\", \"TrainingData/TrainingFeatures/WavLM/AdultVC\", feature_type=\"wavlm\")\n",
    "print(f\"WavLM AdultVC Template Accuracy: {accuracyWavLMAdultVC:.2f}%\")\n",
    "accuracyWavLMChild = getAccuracy(\"ValidationData/OnlyNumbers/\", \"TrainingData/TrainingFeatures/WavLM/Child\", feature_type=\"wavlm\")\n",
    "print(f\"WavLM Child Template Accuracy: {accuracyWavLMChild:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8aba9",
   "metadata": {},
   "source": [
    "MFCCs Accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89197a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCCs Adult Template Accuracy: 23.53%\n",
      "MFCCs AdultVC Template Accuracy: 39.22%\n",
      "MFCCs Child Template Accuracy: 59.80%\n"
     ]
    }
   ],
   "source": [
    "accuracyMFCCsAdult = getAccuracy(\"ValidationData/OnlyNumbers/\", \"TrainingData/TrainingFeatures/MFCCs/Adult\", feature_type=\"mfcc\")\n",
    "print(f\"MFCCs Adult Template Accuracy: {accuracyMFCCsAdult:.2f}%\")\n",
    "accuracyMFCCsAdultVC = getAccuracy(\"ValidationData/OnlyNumbers/\", \"TrainingData/TrainingFeatures/MFCCs/AdultVC\", feature_type=\"mfcc\")\n",
    "print(f\"MFCCs AdultVC Template Accuracy: {accuracyMFCCsAdultVC:.2f}%\")\n",
    "accuracyMFCCsChild = getAccuracy(\"ValidationData/OnlyNumbers/\", \"TrainingData/TrainingFeatures/MFCCs/Child\", feature_type=\"mfcc\")\n",
    "print(f\"MFCCs Child Template Accuracy: {accuracyMFCCsChild:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0983c",
   "metadata": {},
   "source": [
    "## Evaluation for Different WavLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2723c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WavLM Large Accuracy: 65.69%\n"
     ]
    }
   ],
   "source": [
    "accuracyWavLMLarge = getAccuracy(\"ValidationData/OnlyNumbers/\", \"TrainingData/TrainingFeatures/WavLM/Child\", feature_type=\"wavlm\")\n",
    "print(f\"WavLM Large Accuracy: {accuracyWavLMLarge:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047ef32",
   "metadata": {},
   "source": [
    "The large model took 20min 57.9 seconds to run, giving it a prediction time of 12.33 seconds per query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca16e2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WavLM Base+ Template Accuracy: 75.49%\n"
     ]
    }
   ],
   "source": [
    "model = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\").to(device).eval()\n",
    "accuracyWavLMBasePlus = getAccuracy(\"ValidationData/OnlyNumbers/\", \"TrainingData/TrainingFeatures/WavLMBase+/English\", feature_type=\"wavlm\")\n",
    "print(f\"WavLM Base+ Template Accuracy: {accuracyWavLMBasePlus:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d32d3",
   "metadata": {},
   "source": [
    "The Base+ model took 7min 7.3 seconds to run, giving it a prediction time of 4.20 seconds per query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f55a752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WavLM Base Template Accuracy: 57.84%\n"
     ]
    }
   ],
   "source": [
    "model = WavLMModel.from_pretrained(\"microsoft/wavlm-base\").to(device).eval()\n",
    "accuracyWavLMBase = getAccuracy(\"ValidationData/OnlyNumbers/\", \"TrainingData/TrainingFeatures/WavLMBase/English\", feature_type=\"wavlm\")\n",
    "print(f\"WavLM Base Template Accuracy: {accuracyWavLMBase:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e93b2",
   "metadata": {},
   "source": [
    "The base model took 6min 45.8 seconds to run, giving it a prediction time of 3.98 seconds per query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
