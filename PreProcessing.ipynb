{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38782d1b",
   "metadata": {},
   "source": [
    "# Preprocessing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43453a",
   "metadata": {},
   "source": [
    "This code includes the audio pre-processing as well as the feature extraction for the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce30d6",
   "metadata": {},
   "source": [
    "## Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import os, glob, torch,torchaudio, re\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import mfcc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import sys\n",
    "import speech_dtw.qbe as qbe\n",
    "\n",
    "from transformers import WavLMModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(path.join(\"..\", \"utils\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfa988",
   "metadata": {},
   "source": [
    "## Pre-processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing silence and resampling recordings\n",
    "SAMPLE_RATE = 16000 \n",
    "\n",
    "# Load Silero VAD\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=False,\n",
    "                              trust_repo=True)\n",
    "(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils\n",
    "\n",
    "def preProcessAudio(folder):\n",
    "    for wav_fn in Path(folder).rglob(\"*.wav\"): #Loop through all .wav files in folder and subfolders\n",
    "        wav = read_audio(str(wav_fn), sampling_rate=SAMPLE_RATE)  # loads & resamples to 16kHz mono\n",
    "        ts = get_speech_timestamps(\n",
    "            wav, model, sampling_rate=SAMPLE_RATE,\n",
    "            threshold=0.35,\n",
    "            speech_pad_ms=300,\n",
    "            min_speech_duration_ms=150,\n",
    "            min_silence_duration_ms=300,\n",
    "        ) #Gets which timestamps have speech\n",
    "        if ts:  # If there is speech detected, save it\n",
    "            wav_clean = collect_chunks(ts, wav)\n",
    "            save_audio(str(wav_fn), wav_clean, sampling_rate=SAMPLE_RATE)  # overwrite\n",
    "            print(\"Processed\", wav_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78240c8",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595207eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcessAudio(\"TrainingData/Child/Afrikaans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf223466",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800993b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAVLM_LAYER_INDEX = 6\n",
    "\n",
    "# Select the WavLM model to be used here\n",
    "device = \"cpu\"\n",
    "model = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\").to(device).eval()\n",
    "\n",
    "def cmvn(X):\n",
    "    # X: [T, D] NumPy\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True)\n",
    "    return (X - mu) / (sd + 1e-8)\n",
    "\n",
    "def getMFCCsFeatures(file): #A function which extracts MFCCs features from a given audio file\n",
    "    sig, rate = torchaudio.load(file) #Reads the audio file, extracting the sample rate and signal data (as an array)\n",
    "\n",
    "    #Check if sampled as correct sampling rate, if not - resample\n",
    "    if rate != SAMPLE_RATE:\n",
    "        print(\"Resampling\", file ,\"at 16kHz.\\n\")\n",
    "        sig = torchaudio.functional.resample(sig, rate, SAMPLE_RATE)\n",
    "\n",
    "    sig = sig.squeeze(0).numpy()\n",
    "\n",
    "    #Extract features\n",
    "    MFCC_static = mfcc(sig, SAMPLE_RATE) #Extracts MFCCs features given\n",
    "    MFCC_deltas = delta(MFCC_static, 2) #Calculates delta (first derivative) of MFCCs features\n",
    "    MFCC_delta_deltas = delta(MFCC_deltas, 2) #Calculates delta-delta (second derivative) of MFCCs features\n",
    "    \n",
    "    #Combine static, delta, and delta-delta features into a single feature vector\n",
    "    features = np.hstack((MFCC_static, MFCC_deltas, MFCC_delta_deltas))\n",
    "    features = cmvn(features) #Applies cepstral mean and variance normalization to features\n",
    "\n",
    "    return features\n",
    "\n",
    "def getWavLMFeatures(file): #A function which extracts MFCCs features from a given audio file\n",
    "    sig, rate = torchaudio.load(file) #Reads the audio file, extracting the sample rate and signal data (as an array)\n",
    "\n",
    "    #Check if sampled as correct sampling rate, if not - resample\n",
    "    if rate != SAMPLE_RATE:\n",
    "        print(\"Resampling\", file ,\"at 16kHz.\\n\")\n",
    "        sig = torchaudio.functional.resample(sig, rate, SAMPLE_RATE)\n",
    "\n",
    "    #Extracts layer 6 features\n",
    "    sig = sig.to(device)\n",
    "    with torch.inference_mode():\n",
    "        out = model(sig, output_hidden_states=True)\n",
    "        features = out.hidden_states[WAVLM_LAYER_INDEX].squeeze(0)  # [T, D] torch\n",
    "\n",
    "    #Convert to numpy\n",
    "    features = features.numpy()\n",
    "\n",
    "    #Apply CMVN\n",
    "    features = cmvn(features) #Applies cepstral mean and variance normalization to features\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90435275",
   "metadata": {},
   "source": [
    "### Function to save the features as .pt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f613ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAndSaveFeatures(inputPath, outputPath, FeatureType): #Saves all features so they do not have to be re-extracted each time\n",
    "    inputPath = Path(inputPath)\n",
    "    outputPath  = Path(outputPath)\n",
    "\n",
    "    #Expects naming to go A01/01_00.wav, where A01 is the speaker ID and 01 is the label\n",
    "    #A02/01_02.wav would refer to adult speaker 2, where 01 is the label, and 02 is the utterance number (utterance 3)\n",
    "    filePattern = re.compile(r\"^\\d{2}_\\d{2}\\.wav$\")\n",
    "\n",
    "    for wav_fn in inputPath.rglob(\"*.wav\"):  # Loop through all .wav files\n",
    "        fileName = wav_fn.stem                   # e.g. \"03_01\" or \"noNum_01\"\n",
    "        prefix = fileName.split(\"_\")[0]          # \"03\" or \"noNum\"\n",
    "\n",
    "        # Handle label\n",
    "        if prefix.isdigit():\n",
    "            label = int(prefix)\n",
    "        else:\n",
    "            label = \"No Number\"  # e.g. \"noNum_01.wav\"  \n",
    "\n",
    "        if FeatureType == \"MFCCs\":\n",
    "            # Extract MFCC features\n",
    "            features = getMFCCsFeatures(str(wav_fn)) #Gets MFCC features as numpy array\n",
    "            features = torch.from_numpy(features).float() #Convert to torch tensor\n",
    "        elif FeatureType == \"WavLM\":\n",
    "            # Extract WavLM features\n",
    "            features = getWavLMFeatures(str(wav_fn)) #Gets WavLM features as numpy array\n",
    "            features = torch.from_numpy(features).float() #Convert to torch tensor\n",
    "\n",
    "        # Build save path\n",
    "        rel_path = wav_fn.relative_to(inputPath)   # e.g. A01/03_00.wav\n",
    "        save_path = outputPath / rel_path.with_suffix(\".pt\")\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Package data\n",
    "        data = {\n",
    "            \"features\": features,\n",
    "            \"label\": label,\n",
    "            \"speaker\": wav_fn.parent.name,\n",
    "            \"utt_id\": fileName,\n",
    "            \"feat_type\": FeatureType\n",
    "        }\n",
    "\n",
    "        torch.save(data, save_path)\n",
    "        print(f\"Saved {save_path}\")\n",
    "\n",
    "    print(\"Feature extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edf37a",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractAndSaveFeatures(\"TrainingData/Child/Afrikaans\", \"TrainingFeatures/WavLMBase+/Afrikaans\", \"WavLM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
